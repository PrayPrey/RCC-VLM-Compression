# Training Configuration for RCC Experiments
# Knowledge distillation, optimization, and distributed training settings

# General training settings
training:
  # Training mode: distillation | standard | adversarial
  mode: distillation
  # Maximum number of epochs
  max_epochs: 100
  # Maximum training steps (overrides epochs if set)
  max_steps: -1
  # Gradient accumulation steps (effective batch = batch_size * accum * gpus)
  gradient_accumulation_steps: 4
  # Batch size per GPU
  batch_size: 64  # Effective batch size = 64 * 4 * 4 = 1024
  # Evaluation batch size
  eval_batch_size: 128
  # Number of workers for data loading
  num_workers: 8
  # Pin memory for faster GPU transfer
  pin_memory: true
  # Deterministic training for reproducibility
  deterministic: true
  # Random seed
  seed: 42
  # Resume from checkpoint
  resume_from_checkpoint: null
  # Checkpoint saving frequency (in steps)
  save_steps: 1000
  # Evaluation frequency (in steps)
  eval_steps: 500
  # Logging frequency (in steps)
  logging_steps: 100
  # Early stopping patience (in eval steps)
  early_stopping_patience: 10
  # Early stopping metric
  early_stopping_metric: "eval_loss"
  # Early stopping mode: min | max
  early_stopping_mode: min

# Knowledge distillation configuration
distillation:
  # Enable knowledge distillation
  enabled: true
  # Teacher model configuration
  teacher:
    # Teacher model path or identifier
    model_path: "${TEACHER_MODEL_PATH}"
    # Load teacher from checkpoint
    checkpoint: null
    # Freeze teacher model
    freeze: true
    # Teacher model device placement
    device: "cuda:0"
    # Use teacher in eval mode
    eval_mode: true

  # Distillation loss configuration
  loss:
    # Distillation loss type: kl | mse | cosine | combined
    type: combined
    # Temperature for softmax
    temperature: 4.0
    # Temperature scheduling: constant | linear | cosine
    temperature_schedule: cosine
    # Final temperature (for scheduling)
    final_temperature: 1.0
    # Alpha weight for distillation loss
    alpha: 0.7
    # Beta weight for student loss
    beta: 0.3

  # Feature distillation
  feature_distillation:
    # Enable feature-level distillation
    enabled: true
    # Layers to distill from
    layers: ["layer_4", "layer_8", "layer_11", "final"]
    # Feature matching loss: mse | l1 | cosine
    matching_loss: mse
    # Feature loss weight
    weight: 0.2
    # Normalize features before matching
    normalize: true

  # Attention distillation
  attention_distillation:
    # Enable attention map distillation
    enabled: true
    # Attention loss weight
    weight: 0.1
    # Attention matching loss: kl | mse | cosine
    matching_loss: kl
    # Distill self-attention
    self_attention: true
    # Distill cross-attention (for multimodal)
    cross_attention: true

  # Relation distillation
  relation_distillation:
    # Enable relation/similarity distillation
    enabled: false
    # Relation type: instance | feature | angle
    type: instance
    # Relation loss weight
    weight: 0.05

  # Progressive distillation
  progressive:
    # Enable progressive distillation
    enabled: true
    # Start with full distillation
    initial_weight: 1.0
    # Final distillation weight
    final_weight: 0.3
    # Schedule: linear | exponential | step
    schedule: linear
    # Warmup steps for progressive distillation
    warmup_steps: 5000

# Optimizer configuration
optimizer:
  # Optimizer type: adamw | adam | sgd | lamb | adafactor
  type: adamw
  # Learning rate
  learning_rate: 5e-5
  # Weight decay
  weight_decay: 0.01
  # Adam beta1
  beta1: 0.9
  # Adam beta2
  beta2: 0.999
  # Adam epsilon
  epsilon: 1e-8
  # Gradient clipping value
  gradient_clip_val: 1.0
  # Gradient clipping norm
  gradient_clip_norm: 2.0
  # Use 8-bit Adam optimizer
  use_8bit_adam: false
  # Layer-wise learning rate decay
  layer_decay: 0.95
  # Different learning rates for different components
  differential_lr:
    # Enable differential learning rates
    enabled: true
    # Vision encoder learning rate multiplier
    vision_lr_mult: 0.5
    # Text encoder learning rate multiplier
    text_lr_mult: 1.0
    # Projection layer learning rate multiplier
    projection_lr_mult: 2.0
    # Compression module learning rate multiplier
    compression_lr_mult: 1.5

# Learning rate scheduler configuration
scheduler:
  # Scheduler type: cosine | linear | polynomial | constant | cosine_with_restarts
  type: cosine
  # Number of warmup steps
  warmup_steps: 5000
  # Warmup ratio (overrides warmup_steps if set)
  warmup_ratio: 0.05
  # Number of training steps (for linear/polynomial)
  num_training_steps: 100000
  # Number of cycles (for cosine)
  num_cycles: 1
  # Minimum learning rate
  min_lr: 1e-7
  # Maximum learning rate (for cyclic schedules)
  max_lr: 5e-4
  # Cosine annealing specific
  cosine:
    # Restart period (for cosine_with_restarts)
    restart_period: 10000
    # Restart multiplier
    restart_mult: 1.5
    # Eta min for cosine annealing
    eta_min: 1e-7

# Mixed precision training
mixed_precision:
  # Enable mixed precision training
  enabled: true
  # Precision: fp16 | bf16 | fp32
  precision: fp16
  # Automatic mixed precision level: O0 | O1 | O2 | O3
  amp_level: "O1"
  # Loss scaling
  loss_scale:
    # Loss scale type: dynamic | static
    type: dynamic
    # Initial scale
    init_scale: 65536
    # Growth factor
    growth_factor: 2.0
    # Backoff factor
    backoff_factor: 0.5
    # Growth interval
    growth_interval: 2000
  # Operations to keep in fp32
  fp32_ops: ["layer_norm", "softmax", "loss"]

# Distributed training configuration
distributed:
  # Enable distributed training
  enabled: true
  # Number of GPUs to use
  num_gpus: 4
  # Distributed backend: nccl | gloo | mpi
  backend: nccl
  # Local rank (set automatically)
  local_rank: -1
  # World size (set automatically)
  world_size: -1
  # Node rank for multi-node training
  node_rank: 0
  # Number of nodes
  num_nodes: 1
  # Master address for multi-node
  master_addr: "localhost"
  # Master port
  master_port: 29500
  # Distributed data parallel
  ddp:
    # Find unused parameters
    find_unused_parameters: false
    # Bucket cap in MB
    bucket_cap_mb: 25
    # Gradient as bucket view
    gradient_as_bucket_view: true
    # Static graph optimization
    static_graph: false
    # Broadcast buffers
    broadcast_buffers: true
  # Fully sharded data parallel (FSDP)
  fsdp:
    # Enable FSDP
    enabled: false
    # Sharding strategy: full_shard | shard_grad_op | no_shard
    sharding_strategy: full_shard
    # Min params to wrap
    min_num_params: 1e6
    # CPU offload
    cpu_offload: false
    # Mixed precision
    mixed_precision: true

# Gradient accumulation and optimization
gradient_optimization:
  # Gradient accumulation
  accumulation:
    # Number of accumulation steps
    steps: 4
    # Normalize by accumulation steps
    normalize: true
    # Sync gradients at each step
    sync_each_step: false

  # Gradient checkpointing
  checkpointing:
    # Enable gradient checkpointing
    enabled: true
    # Checkpoint segments
    segments: 2
    # Checkpoint specific layers
    checkpoint_layers: ["attention", "ffn"]

  # Gradient penalty and regularization
  regularization:
    # L2 regularization
    l2_weight: 1e-4
    # L1 regularization
    l1_weight: 0
    # Gradient penalty weight
    gradient_penalty: 0
    # Spectral normalization
    spectral_norm: false

# Training strategies
strategies:
  # Curriculum learning
  curriculum:
    # Enable curriculum learning
    enabled: false
    # Curriculum type: difficulty | size | noise
    type: difficulty
    # Number of curriculum stages
    num_stages: 3
    # Stage transition: linear | step | exponential
    transition: linear

  # Data augmentation during training
  augmentation:
    # Enable augmentation
    enabled: true
    # Augmentation probability
    prob: 0.5
    # Mixup
    mixup:
      enabled: true
      alpha: 0.2
    # CutMix
    cutmix:
      enabled: false
      alpha: 1.0
    # Random erasing
    random_erasing:
      enabled: true
      prob: 0.25

  # Adversarial training
  adversarial:
    # Enable adversarial training
    enabled: false
    # Attack type: fgsm | pgd | none
    attack_type: pgd
    # Epsilon for adversarial perturbation
    epsilon: 0.01
    # Number of attack iterations
    num_iter: 10
    # Step size for iterative attacks
    step_size: 0.003

# Memory optimization
memory:
  # Memory efficient attention
  efficient_attention:
    # Enable efficient attention
    enabled: true
    # Attention implementation: flash | xformers | standard
    implementation: flash
    # Memory efficient backward pass
    efficient_backward: true

  # CPU offloading
  cpu_offload:
    # Enable CPU offloading
    enabled: false
    # Offload optimizer states
    offload_optimizer: true
    # Offload model parameters
    offload_params: false

  # Memory monitoring
  monitoring:
    # Monitor GPU memory
    enabled: true
    # Log memory usage frequency (in steps)
    log_frequency: 100
    # Alert on high memory usage (fraction)
    alert_threshold: 0.95

# Validation and evaluation
evaluation:
  # Evaluation strategy: steps | epoch | no
  strategy: steps
  # Evaluation steps
  eval_steps: 500
  # Metrics to compute
  metrics:
    - accuracy
    - loss
    - perplexity
    - bleu
    - rouge
    - clip_score
  # Save best model
  save_best: true
  # Best model metric
  best_metric: "eval_loss"
  # Best model mode: min | max
  best_mode: min
  # Compute metrics on training set
  eval_on_train: false

# Checkpointing configuration
checkpointing:
  # Save strategy: steps | epoch | best
  save_strategy: steps
  # Save steps
  save_steps: 1000
  # Save total limit (number of checkpoints)
  save_total_limit: 5
  # Save only model (not optimizer/scheduler)
  save_only_model: false
  # Save optimizer and scheduler
  save_optimizer: true
  # Load best model at end
  load_best_at_end: true
  # Checkpoint format: pytorch | safetensors
  format: safetensors

# Logging configuration
logging:
  # Logging level: debug | info | warning | error
  level: info
  # Loggers to use: tensorboard | wandb | mlflow | csv
  loggers:
    - tensorboard
    - wandb
  # TensorBoard configuration
  tensorboard:
    # Log directory
    log_dir: "${OUTPUT_DIR}/tensorboard"
    # Histogram frequency
    histogram_freq: 100
    # Profile batch
    profile_batch: "100,110"
  # Weights & Biases configuration
  wandb:
    # Project name
    project: "rcc_experiments"
    # Entity/team name
    entity: null
    # Run name
    name: "${EXPERIMENT_NAME}"
    # Tags
    tags: ["rcc", "compression", "vision-language"]
    # Log model
    log_model: true
    # Log gradients
    log_gradients: true
    # Gradient log frequency
    gradient_log_freq: 100
  # MLflow configuration
  mlflow:
    # Tracking URI
    tracking_uri: "file://${OUTPUT_DIR}/mlruns"
    # Experiment name
    experiment_name: "rcc_compression"

# Hardware-specific optimizations for A100
hardware:
  # GPU type
  gpu_type: "A100"
  # Enable TF32 for A100
  tf32: true
  # Enable tensor cores
  tensor_cores: true
  # CUDA specific settings
  cuda:
    # CUDA matmul precision: high | medium | low
    matmul_precision: high
    # Enable CUDNN benchmark
    cudnn_benchmark: true
    # CUDNN deterministic
    cudnn_deterministic: false
    # Flash attention
    flash_attention: true
    # Memory fraction
    memory_fraction: 0.95

# Hyperparameter ranges for Bayesian optimization
hyperparameter_ranges:
  learning_rate: [1e-6, 1e-4]
  batch_size: [32, 128]
  gradient_accumulation_steps: [1, 8]
  warmup_ratio: [0.01, 0.1]
  weight_decay: [0.0, 0.1]
  distillation_alpha: [0.5, 0.9]
  distillation_temperature: [1.0, 10.0]
  dropout: [0.0, 0.3]
  label_smoothing: [0.0, 0.2]